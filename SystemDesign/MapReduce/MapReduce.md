# MapReduce

A popular framework for processing very large datasets in a distributed setting efficiently, quickly, and in a fault-tolerant manner. A MapReduce job is comprised of 3 main steps:

- the **Map** step, which runs a **map function** on the various chunks of the dataset and transforms these chunks into intermediate **key-value pairs**

- the **Shuffle** step, which reorgnaizes the intermediate **key-value pairs** such that pairs of the same key are routed to the same machine in the final step.
- the **Reduce** step, which runs a **reduce function** on the newly shuffled **key-value pairs** and transforms them into more meaningful data.

The canonical example of a MapReduce use case is counting the numer of occurrences of words in a large text file.

When dealing with a MapReduce library, engineers and/or systems administrators only need to worry about the map and reduce functions, as well as their inputs and outputs. All other concerns, including the paralleization of tasks and the fault-tolerance of the MapReduce job, are abstrated away and taken care of by the MapReduce implementation.

## Distributed File System

A Distributed File Sytem is a abstration over a (usually large) cluster of machines that allows them to act like one large file system. The two most poopular implementations of a DFS are the **Google File System** (GFS) and the **Hadopp Distributed File System (HDFS)**.

Typically, DFSs take care of the classic **availability** and **replication** guarantees that can be tricky to obtain in a distributed-system setting. The overaching idea is that files are split into chunks of a certain size (4MB or 64MB, for instance), and those chunks are sharded across a large cluster of machines. A central control plane is in charge of deciding where each chunk resides, routing reads to the right nodes, and handling communication between machines.

Different DFS implementations have slightly different APIs and sematics, but they achieve the same common goal: extremely large-scale persistent storage.
